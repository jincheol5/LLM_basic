import os
from llm import DataUtils
from peft import LoraConfig,get_peft_model
from transformers import Trainer,TrainingArguments,DataCollatorForLanguageModeling

# llm_pipeline=DataUtils.load_local_llm_pipeline(model_type=f"pretrained",model_name=f"Qwen2.5-1.5B-Instruct",task=f"text-generation")
# output=llm_pipeline(f"ì„¸ì¢…ëŒ€í•™êµì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.",max_new_tokens=100,return_full_text=False)
# print(output[0]["generated_text"])

model,tokenizer=DataUtils.load_local_llm(model_type=f"pretrained",model_name=f"Qwen2.5-1.5B-Instruct")
model.config.use_cache=False  # Trainer + gradient checkpointing ì•ˆì •ì„±

dataset=DataUtils.load_dataset(dataset_name=f"KoAlpaca-v1.1a")
dataset=dataset['train']

def preprocess_simple(example):
    prompt=example["instruction"]
    if example.get("input") and example["input"].strip()!="":
        prompt+="\n"+example["input"]
    text=prompt+"\n"+example["output"]
    return {"text":text}

dataset=dataset.map(
    preprocess_simple,
    remove_columns=dataset.column_names
)

lora_config=LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
    ]
)

model=get_peft_model(model,lora_config)
model.print_trainable_parameters()  # trainable params í™•ì¸

# ================================
# DataCollator (Causal LM)
# ================================
data_collator=DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ================================
# TrainingArguments ì„¤ì •
# ================================
OUTPUT_DIR=os.path.join('..','data','llm_basic','adapter','Adapter_LoRA_llm_Qwen2.5-1.5B-Instruct_dataset_KoAlpaca-v1.1a')

training_args=TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=3,
    bf16=True,                     # bf16 ì•ˆ ë˜ë©´ fp16=True
    logging_steps=50,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none",
    remove_unused_columns=False
)


# ================================
# Trainer ìƒì„± -> modelì´ ìˆëŠ” ë””ë°”ì´ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
# ================================
trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ================================
# LoRA íŒŒì¸íŠœë‹ ì‹œì‘
# ================================
trainer.train()


# ================================
# LoRA adapter ì €ì¥ (ğŸ”¥ ì¤‘ìš”)
# ================================
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)